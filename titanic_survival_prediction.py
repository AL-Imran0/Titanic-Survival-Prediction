# -*- coding: utf-8 -*-
"""Titanic Survival Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10sH6y6p_Awo20x-2pvMIOj8xBuQzusHm
"""

# Install necessary libraries
!pip install seaborn pandas matplotlib scikit-learn xgboost

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix
import xgboost as xgb



df = sns.load_dataset('titanic')


if 'name' not in df.columns:
    df['name'] = ['Unknown'] * len(df)


columns_to_drop = ['deck', 'embark_town', 'alive', 'class', 'who', 'boat', 'body', 'cabin']
existing_columns_to_drop = [col for col in columns_to_drop if col in df.columns]
df.drop(existing_columns_to_drop, axis=1, inplace=True)


df.dropna(subset=['embarked'], inplace=True)
df['age'].fillna(df['age'].median(), inplace=True)
df['embarked'].fillna(df['embarked'].mode()[0], inplace=True)

# Feature Engineering
df['family_size'] = df['sibsp'] + df['parch']  # Total family size
df['is_alone'] = (df['family_size'] == 0).astype(int)  # Whether the passenger is alone or not
df['age_group'] = pd.cut(df['age'], bins=[0, 12, 18, 35, 60, 100], labels=['Child', 'Teenager', 'Adult', 'Middle_Aged', 'Senior'])
df['fare_per_person'] = df['fare'] / (df['family_size'] + 1)  # Fare per person


def extract_title(name):
    try:

        return name.split(',')[1].split('.')[0].strip()
    except:
        return 'Unknown'

df['title'] = df['name'].apply(extract_title)


df = pd.get_dummies(df, columns=['sex', 'embarked', 'age_group', 'title'], drop_first=True)


df['age'].fillna(df['age'].median(), inplace=True)
df['fare'].fillna(df['fare'].median(), inplace=True)


df.drop(columns=['ticket', 'passenger_id'], errors='ignore', inplace=True)


X = df.drop(columns=['survived'])
y = df['survived']


X = X.apply(pd.to_numeric, errors='coerce')

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Model: Random Forest Classifier
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Predictions
y_pred = model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy * 100:.2f}%")

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Died', 'Survived'], yticklabels=['Died', 'Survived'])
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()

# Feature Importances
importances = model.feature_importances_
indices = np.argsort(importances)[::-1]

# Visualize feature importance
plt.figure(figsize=(12, 6))
plt.title("Feature Importance")
plt.barh(range(X.shape[1]), importances[indices], align="center")
plt.yticks(range(X.shape[1]), X.columns[indices])
plt.xlabel("Relative Importance")
plt.show()

# Additional Visualizations

# Survival by family size
sns.countplot(x='family_size', hue='survived', data=df)
plt.title("Survival by Family Size")
plt.xlabel("Family Size")
plt.ylabel("Count")
plt.legend(["Died", "Survived"])
plt.show()

# Survival by Fare per Person
sns.boxplot(x='survived', y='fare_per_person', data=df)
plt.title("Survival by Fare per Person")
plt.xlabel("Survived")
plt.ylabel("Fare per Person")
plt.show()

# Age group survival
sns.countplot(x='age_group_Adult', hue='survived', data=df)
plt.title("Survival by Age Group")
plt.xlabel("Age Group (Adult)")
plt.ylabel("Count")
plt.show()

# Model Testing with more advanced models (XGBoost)

# XGBoost Model
xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)
xgb_model.fit(X_train, y_train)
xgb_pred = xgb_model.predict(X_test)
xgb_accuracy = accuracy_score(y_test, xgb_pred)
print(f"XGBoost Accuracy: {xgb_accuracy * 100:.2f}%")